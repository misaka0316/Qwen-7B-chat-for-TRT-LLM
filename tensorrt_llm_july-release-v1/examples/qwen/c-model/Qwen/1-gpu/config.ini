[qwen]
out_dir = ./c-model/Qwen
in_file = Qwen/Qwen-7B-Chat
tensor_parallelism = 1
processes = 4
calibrate_kv_cache = False
smoothquant = None
model = Qwen
storage_type = float16
dataset_cache_dir = None
vocab_size = 151936
hidden_size = 4096
intermediate_size = 22016
num_hidden_layers = 32
num_attention_heads = 32
emb_dropout_prob = 0.0
attn_dropout_prob = 0.0
layer_norm_epsilon = 1e-06
initializer_range = 0.02
scale_attn_weights = True
use_cache = True
max_position_embeddings = 8192
bf16 = False
fp16 = True
fp32 = False
kv_channels = 128
rotary_pct = 1.0
rotary_emb_base = 10000
use_dynamic_ntk = True
use_logn_attn = True
use_flash_attn = True
no_bias = True
return_dict = True
output_hidden_states = False
output_attentions = False
torchscript = False
torch_dtype = None
use_bfloat16 = False
tf_legacy_loss = False
pruned_heads = {}
tie_word_embeddings = False
is_encoder_decoder = False
is_decoder = False
cross_attention_hidden_size = None
add_cross_attention = False
tie_encoder_decoder = False
max_length = 20
min_length = 0
do_sample = False
early_stopping = False
num_beams = 1
num_beam_groups = 1
diversity_penalty = 0.0
temperature = 1.0
top_k = 50
top_p = 1.0
typical_p = 1.0
repetition_penalty = 1.0
length_penalty = 1.0
no_repeat_ngram_size = 0
encoder_no_repeat_ngram_size = 0
bad_words_ids = None
num_return_sequences = 1
chunk_size_feed_forward = 0
output_scores = False
return_dict_in_generate = False
forced_bos_token_id = None
forced_eos_token_id = None
remove_invalid_values = False
exponential_decay_length_penalty = None
suppress_tokens = None
begin_suppress_tokens = None
architectures = ['QWenLMHeadModel']
finetuning_task = None
id2label = {0: 'LABEL_0', 1: 'LABEL_1'}
label2id = {'LABEL_0': 0, 'LABEL_1': 1}
tokenizer_class = None
prefix = None
bos_token_id = None
pad_token_id = None
eos_token_id = None
sep_token_id = None
decoder_start_token_id = None
task_specific_params = None
problem_type = None
_name_or_path = Qwen/Qwen-7B-Chat
_commit_hash = b9ccaf8faff835f2017088795432aca32055f6af
transformers_version = 4.32.0
auto_map = {'AutoConfig': 'Qwen/Qwen-7B-Chat--configuration_qwen.QWenConfig', 'AutoModelForCausalLM': 'Qwen/Qwen-7B-Chat--modeling_qwen.QWenLMHeadModel'}
model_type = qwen
onnx_safe = None
seq_length = 2048
tokenizer_type = QWenTokenizer
storage_dtype = float16
multi_query_mode = False

